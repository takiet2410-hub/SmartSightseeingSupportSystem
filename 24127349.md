# ğŸ§­Interest + Activities Recommendation Pipeline â€“ Smart Tourism System

## 1ï¸âƒ£ Objective

This pipeline processes users' **intentions and preferences** â€“ for example:

> â€œI want to go somewhere peaceful, with lots of greenery, a cool climate, and swimming in the sea.â€

The objective is to **find the most emotionally and activity-aligned travel destinations** in the database (DB) using the model **SentenceTransformer + TF-IDF + Cosine Similarity + KNN**.

---

## 2ï¸âƒ£ Pipeline Overview

```text
User enters vibe
â†“
Vectorization (TF-IDF + SentenceTransformer)
â†“
Similarity calculation (Cosine Similarity)
â†“
Find top nearest locations (KNN / Ranking)
â†“
Return relevant location suggestions
```

---

## 3ï¸âƒ£ Detailed Components

### ğŸ”¹ 3.1 TF-IDF (Term Frequency - Inverse Document Frequency)

TF-IDF evaluates the importance of a word in a text corpus.

$$
TF\text{-}IDF(t, d) = TF(t, d) \times \log\left(\frac{N}{DF(t)}\right)
$$

- \(TF(t, d)\): frequency of word \(t\) in document \(d\)  
- \(DF(t)\): number of documents containing word \(t\)  
- \(N\): total number of documents  

TF-IDF helps the model identify **distinctive keywords** for each location.
---

### ğŸ”¹ 3.2 SentenceTransformer (Semantic Embedding)

#### ğŸ§  Overview

**SentenceTransformer** is a deep learning model based on **BERT (Bidirectional Encoder Representations from Transformers)**, fine-tuned to generate vector representations (embeddings) for entire sentences or paragraphs â€” rather than individual words.

The model aims to map sentences with **similar meanings** to **points close to each other in vector space**.

#### ğŸ”¬ General Structure
```text
Input string (sentence)
â†“
Tokenizer (converts words â†’ token IDs)
â†“
Transformer Encoder (BERT / MiniLM / DistilBERT)
â†“
Pooling Layer (Mean / Max / CLS token)
â†“
Sentence Embedding (semantic vector)
```

Each sentence after passing through the model is represented by a vector with 384â€“768 dimensions (depending on the model), for example, `multilingual-e5-base` creates a 384-dimensional vector.

#### âš™ï¸ Detailed Working Mechanism

1. **Tokenizer**  
   Converts the input sentence into a string of token IDs, for example:  
   `â€œÄÃ  Láº¡t yÃªn bÃ¬nhâ€ â†’ [101, 3912, 1652, 102]`  
   (encoding based on WordPiece Tokenization)

2. **Transformer Encoder**  
   Applies *multi-head self-attention*, allowing the model to capture relationships between words in a bidirectional context:

  `Attention(Q, K, V) = softmax( (QKáµ€) / âˆšdâ‚– ) Ã— V`

   Where:
   - \(Q, K, V\): query, key, value matrices  
   - \(d\_k\): hidden size

3. **Pooling Layer**  
   Averages (mean pooling) all token embeddings into a single vector:

`s = (1/n) Î£áµ¢â‚Œâ‚â¿ háµ¢`


where \(h\_i\) is the embedding of the *i*th token.

4. **Sentence Embedding**  
   The result is a semantic vector representing the entire sentence, for example:
   [0.123, -0.041, 0.332, ... , 0.027]
   
#### ğŸ§© In the Vibe Recommendation Problem

In smart tourism systems, SentenceTransformer helps:
- Understand the **deep meaning** of location descriptions (â€œmisty cityâ€ â‰ˆ â€œcool weather, foggyâ€).
- Understand **abstract vibes** from users (â€œhealing,â€ â€œpeaceful,â€ â€œchillâ€).
- Connect different expressions with the same emotional content.

Example:

| User vibe | Location vibe | Cosine Similarity |
|----------------- |---------------|------------------|
| â€œyÃªn tÄ©nh, khÃ­ háº­u mÃ¡t máº»â€ | â€œkhÃ´ng khÃ­ trong lÃ nh, nhiá»u cÃ¢y thÃ´ngâ€ | 0.89 |
| â€œnÃ¡o nhiá»‡t, sÃ´i Ä‘á»™ngâ€ | â€œbiá»ƒn, tiá»‡c, lá»… há»™iâ€ | 0.86 |
| â€œchá»¯a lÃ nhâ€ | â€œspa, thiÃªn nhiÃªn, tÄ©nh tÃ¢mâ€ | 0.81 |

The vector embeddings are normalized and used to calculate **cosine similarity**, enabling the system to suggest travel destinations that match the *mood* rather than just relying on *keywords*.
#### ğŸ’¡ MÃ´ hÃ¬nh sá»­ dá»¥ng

In the current pipeline:
```python
from sentence_transformers import SentenceTransformer
st = SentenceTransformer(â€œmultilingual-e5-baseâ€)
```
---

## 4ï¸âƒ£ Standardizing the hybrid pipeline (TF-IDF + SentenceTransformer)

To leverage the strengths of both models â€” TF-IDF (understanding keywords) and SentenceTransformer (understanding semantics) â€” we combine them into a hybrid vector:

**Combination formula:**

`Vâ‚•áµ§áµ‡Ê³á¶¦áµˆ = [ V_TFIDF ; V_ST ]`

That is, we concatenate the two vectors horizontally.  
For example, if TF-IDF produces a 5000-dimensional vector and SentenceTransformer produces a 384-dimensional vector, then the hybrid vector has a total of 5384 dimensions.

---

## 5ï¸âƒ£ Cosine Similarity

A common similarity measure in vector space is defined by **cosine similarity** â€” measuring the angle between two vectors.

**Formula:**

`cosine_similarity(A, B) = (A âˆ™ B) / (â€–Aâ€– Ã— â€–Bâ€–)`

Values:
- 1 â†’ identical  
- 0 â†’ unrelated  
- -1 â†’ opposite  

---

## 6ï¸âƒ£ KNN (K-Nearest Neighbors)

KNN finds the **k nearest points** to the input vector in cosine space.

`dâ‚cosâ‚(A, B) = 1 âˆ’ cosine_similarity(A, B)`

â†’ Select the **k locations with the smallest distance** to suggest.
---






