## **Problem Analysis: “During” Phase – Computer Vision API Integration**

### **Problem Context**

A tourist is standing in front of a building, statue, or landscape, curious to know what it is, its history, and whether there’s anything worth exploring nearby. Manually searching (e.g., “golden building with curved roof in Hoi An”) is slow and inaccurate.

They need an instant way to **scan** the place and receive accurate information — just by using their camera.

The **Smart Sightseeing Support** system will use **Google Vision AI (Landmark Detection API)** to identify landmarks, then process the returned JSON to extract the **Landmark Name** and **Confidence Score**, and link them to an information database to display results for the tourist.

---

### **1\) Identify Stakeholders**

| Stakeholder | Role / Concern |
| ----- | ----- |
| **Tourist** | Direct user. Wants to instantly know the name and information of the landmark without manual search. |
| **Local Businesses** | Benefit if the result screen shows nearby services (restaurants, tours, souvenir shops). |
| **Monument Management / Cultural Authority** | Want information displayed to be accurate, updated, and historically correct. |
| **Development Team (Developers)** | Need to build a stable, fast system that communicates efficiently with Google’s API. |

---

### **2\) Clarify Objectives**

| Objective Code | Objective | Meaning |
| ----- | ----- | ----- |
| **O1: Speed & Availability** | The system must identify the landmark in under 10 seconds after the image upload. | Minimizes interruption to user experience. |
| **O2: Accuracy & Relevance** | \- Accuracy \> 90% under normal lighting conditions.- Use GPS to avoid confusion between similar landmarks. | Ensures user trust in results. |
| **O3: Engagement** | \- ≥ 60% of users click suggested information after recognition.- Achieve average satisfaction ≥ 4.5/5 (CSAT). | Makes users find the app useful and engaging. |

---

### **3\) Define Inputs and Expected Outputs**

#### **A. Inputs**

| Type | Description |
| ----- | ----- |
| **User Inputs** | Image captured from camera or uploaded (.jpg, .png, .jpeg, .heic). |
| **System Inputs** | \- User GPS coordinates.- Camera orientation data.- Google Vision AI (Landmark Detection) model.- Landmark information database. |

#### **B. Expected Outputs**

| Type | Description |
| ----- | ----- |
| **Primary Outputs** | \- **Landmark Name** – Recognized landmark (e.g., “Notre-Dame Cathedral Basilica of Saigon”).- **Confidence Score** – Prediction confidence (%) (e.g., “98%”). |
| **Supporting Outputs** | \- **Information Summary** – \<100 words.- **Actionable Info** – Opening hours, ticket prices, reviews.- **Interactive Options** – Buttons like “Get Directions,” “Read More,” “Save to Album.” |

---

### **4\) State Constraints**

| Constraint Group | Description |
| ----- | ----- |
| **Technical / Algorithmic** | \- Google Vision AI requires clear, high-resolution images.- Response time depends on network speed.- Angled or obscured photos may reduce recognition accuracy. |
| **Data** | \- Input images must follow standard formats.- Database must be kept updated (ticket prices, opening hours). |
| **Hardware / Network** | \- Requires good camera and stable network (4G/5G or Wi-Fi). |
| **Regulatory / Ethical** | \- Must not detect sensitive locations (military bases, private homes). |
| **User Constraints** | \- App should support an offline fallback mode.- Should not display too much data at once to avoid AR clutter. |

---

### **5\) Technical Decomposition – Implementation of 3.2.1 and 3.2.2**

#### **3.2.1 Write a function to call Google Vision AI (Landmark Detection)**

**Input:** Image file from user (.jpg, .png, .jpeg, .heic).

**Process:**

1. Read the image file → Encode in base64 → Send a POST request to the Google Vision API endpoint (`images:annotate`).

**Payload structure:**

{  
  "requests": \[  
    {  
      "image": {"content": "\<base64\_image\_data\>"},  
      "features": \[{"type": "LANDMARK\_DETECTION"}\]  
    }  
  \]  
}

**Output:** JSON containing a list of landmark candidates and their confidence scores.

---

#### **3.2.2 Write a function to process the returned JSON**

**Input:** JSON from API

**Process:**  
 Access the field `landmarkAnnotations` and extract:

landmark\_name \= response\["landmarkAnnotations"\]\[0\]\["description"\]  
confidence \= response\["landmarkAnnotations"\]\[0\]\["score"\]

**Output:**

{  
  "landmark\_name": "Notre-Dame Cathedral Basilica of Saigon",  
  "confidence": 0.983  
}

---

### **6\) Success Metrics**

| Criterion | Measurement |
| ----- | ----- |
| **Response Time** | \< 10 seconds per image |
| **Recognition Accuracy** | ≥ 90% under good lighting |
| **User Interaction Rate** | ≥ 60% |
| **User Satisfaction (CSAT)** | ≥ 4.5/5 |

---

### **✅ Summary**

The **“During – CV API Integration”** phase is the **AI core** of the entire project, transforming the tourist’s camera into an intelligent recognition tool.

Its success depends on:

* **API speed** (Google Vision),

* **JSON processing accuracy**,

* **User-friendly result presentation**, and

* **Seamless linkage** with the “After” module for saving photos and creating travel albums.