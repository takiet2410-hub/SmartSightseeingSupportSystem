import torch
import torch.nn as nn
from transformers import Dinov2Model, AutoImageProcessor
from PIL import Image
import pandas as pd
import os
import glob  # <--- Th∆∞ vi·ªán quan tr·ªçng ƒë·ªÉ t√¨m file b·∫•t ch·∫•p ƒëu√¥i
from tqdm import tqdm
from core.db import get_mongo_collection

# --- C·∫§U H√åNH IMPORT T·ª™ FILE CONFIG ---
# N·∫øu file config.py c·ªßa b·∫°n thi·∫øu bi·∫øn n√†o, h√£y th√™m v√†o ho·∫∑c ƒë·ªãnh nghƒ©a tr·ª±c ti·∫øp ·ªü ƒë√¢y
try:
    from core.config import CSV_FILE, BATCH_SIZE
except ImportError:
    # Gi√° tr·ªã m·∫∑c ƒë·ªãnh ph√≤ng h·ªù l·ªói import
    CSV_FILE = "VN_train_data.csv"
    BATCH_SIZE = 32

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N LOCAL ---
# C·∫•u tr√∫c th∆∞ m·ª•c: DATASET_ROOT_DIR / landmark_id / image_id.jpg (ho·∫∑c png, webp...)
# V√≠ d·ª•: dataset/train/1/3521.jpg
DATASET_ROOT_DIR = "dataset/train" 

# --- C·∫§U H√åNH MODEL ---
MODEL_NAME = "facebook/dinov2-base"
MODEL_PATH = "models/dinov2_hf_finetuned_ep30.pth" 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üöÄ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}")

# --- 1. KHAI B√ÅO KI·∫æN TR√öC MODEL ---
class FineTunedDINOv2(nn.Module):
    def __init__(self):
        super(FineTunedDINOv2, self).__init__()
        # S·ª≠ d·ª•ng l·∫°i ki·∫øn tr√∫c b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a
        self.backbone = Dinov2Model.from_pretrained(MODEL_NAME)

    def forward(self, x):
        outputs = self.backbone(x)
        cls_token = outputs.last_hidden_state[:, 0]
        # B·∫Øt bu·ªôc Chu·∫©n h√≥a L2 gi·ªëng nh∆∞ khi t·∫°o vector trong DB
        return nn.functional.normalize(cls_token, p=2, dim=1)

def load_finetuned_model():
    # S·ª≠ d·ª•ng l·∫°i logic load model ƒë√£ s·ª≠a ƒë·ªÉ x·ª≠ l√Ω checkpoint dictionary
    print(f"‚è≥ ƒêang t·∫£i model t·ª´ {MODEL_PATH}...")
    model = FineTunedDINOv2()
    
    if os.path.exists(MODEL_PATH):
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)
        
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            print(f"‚ÑπÔ∏è ƒêang tr√≠ch xu·∫•t weights t·ª´ Checkpoint...")
            state_dict = checkpoint["model_state_dict"]
        else:
            state_dict = checkpoint

        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        
        try:
            model.load_state_dict(new_state_dict)
        except:
            # T·∫Øt strict ƒë·ªÉ x·ª≠ l√Ω n·∫øu c√≥ ch√™nh l·ªách nh·ªè v·ªÅ key
            model.load_state_dict(new_state_dict, strict=False)

        print("‚úÖ ƒê√£ load weights th√†nh c√¥ng!")

    else:
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file model t·∫°i {MODEL_PATH}")
    
    model.to(DEVICE)
    model.eval() 
    
    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    return model, processor

def main():
    # 1. K·∫øt n·ªëi MongoDB
    try:
        collection, client = get_mongo_collection()
        print("‚úÖ K·∫øt n·ªëi MongoDB th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói k·∫øt n·ªëi DB: {e}")
        return

    # 2. X√≥a d·ªØ li·ªáu c≈© (Re-indexing)
    # B∆∞·ªõc n√†y quan tr·ªçng v√¨ model ƒë√£ thay ƒë·ªïi, vector c≈© kh√¥ng c√≤n t√°c d·ª•ng
    print("‚ö†Ô∏è ƒêang x√≥a d·ªØ li·ªáu c≈© trong Collection...")
    collection.delete_many({})
    print("‚úÖ ƒê√£ x√≥a s·∫°ch collection. S·∫µn s√†ng n·∫°p m·ªõi.")

    # 3. Load Model & Processor
    try:
        model, processor = load_finetuned_model()
    except Exception as e:
        print(e)
        return

    # 4. ƒê·ªçc file CSV
    if not os.path.exists(CSV_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file CSV: {CSV_FILE}")
        return
    
    df = pd.read_csv(CSV_FILE)
    print(f"üìÇ ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ file CSV.")

    # 5. V√≤ng l·∫∑p x·ª≠ l√Ω ch√≠nh
    documents_batch = []
    missing_files_count = 0
    success_count = 0
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu Embed d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c: {DATASET_ROOT_DIR}")
    print("üëâ Logic: ƒê·ªçc ID t·ª´ CSV -> T√¨m file local (b·∫•t ch·∫•p ƒëu√¥i) -> Embed -> L∆∞u MongoDB")
    
    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Processing"):
        try:
            # L·∫•y th√¥ng tin metadata t·ª´ CSV
            landmark_id = str(row['landmark_id'])
            image_id = str(row['image_id'])
            original_url = row['image_url'] 

            if pd.isna(image_id) or image_id.strip() == "":
                continue

            # --- LOGIC T√åM FILE D√ôNG GLOB (X·ª≠ l√Ω ƒëa ƒë·ªãnh d·∫°ng) ---
            # T·∫°o ƒë∆∞·ªùng d·∫´n m·∫´u: dataset/train/1/3521.*
            # D·∫•u * s·∫Ω kh·ªõp v·ªõi m·ªçi ƒëu√¥i (.jpg, .png, .webp, .JPG...)
            search_path_pattern = os.path.join(DATASET_ROOT_DIR, landmark_id, f"{image_id}.*")
            
            # T√¨m t·∫•t c·∫£ c√°c file kh·ªõp m·∫´u
            found_files = glob.glob(search_path_pattern)

            if not found_files:
                # N·∫øu list r·ªóng nghƒ©a l√† kh√¥ng t√¨m th·∫•y file n√†o kh·ªõp ID n√†y
                # print(f"‚ö†Ô∏è Missing file for ID: {image_id}") # Uncomment n·∫øu mu·ªën xem chi ti·∫øt
                missing_files_count += 1
                continue
            
            # L·∫•y file ƒë·∫ßu ti√™n t√¨m th·∫•y (th∆∞·ªùng ch·ªâ c√≥ 1 file duy nh·∫•t kh·ªõp ID)
            local_image_path = found_files[0]

            # a. M·ªü ·∫£nh t·ª´ ·ªï c·ª©ng
            try:
                # .convert("RGB") l√† b·∫Øt bu·ªôc ƒë·ªÉ tr√°nh l·ªói k√™nh Alpha (trong png) ho·∫∑c Grayscale
                img = Image.open(local_image_path).convert("RGB")
            except Exception as e:
                print(f"‚ùå L·ªói file h·ªèng {local_image_path}: {e}")
                continue

            # b. Vector h√≥a (Embedding)
            inputs = processor(images=img, return_tensors="pt").pixel_values.to(DEVICE)
            
            with torch.no_grad():
                embedding_tensor = model(inputs)
            
            # Chuy·ªÉn Tensor v·ªÅ List chu·∫©n Python
            vector = embedding_tensor.cpu().numpy().flatten().tolist()
            
            # c. T·∫°o Document chu·∫©n c·∫•u tr√∫c
            # QUAN TR·ªåNG: Metadata l·∫•y t·ª´ CSV ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªÉn th·ªã ƒë√∫ng tr√™n Web/App
            doc = {
                "image_id": image_id,
                "landmark_id": landmark_id,
                "image_url": original_url, # V·∫´n l∆∞u URL online
                "embedding": vector
            }
            
            documents_batch.append(doc)
            success_count += 1

            # d. N·∫°p v√†o DB theo l√¥ (Batch insert)
            if len(documents_batch) >= BATCH_SIZE:
                collection.insert_many(documents_batch)
                documents_batch = [] # Reset l√¥

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói kh√¥ng x√°c ƒë·ªãnh t·∫°i d√≤ng {index}: {e}")
            continue

    # 6. N·∫°p n·ªët s·ªë c√≤n l·∫°i trong l√¥ cu·ªëi c√πng
    if documents_batch:
        collection.insert_many(documents_batch)

    # 7. B√°o c√°o k·∫øt qu·∫£
    print("\n" + "="*50)
    print(f"üéâ HO√ÄN T·∫§T QU√Å TR√åNH INGEST!")
    print(f"üìä T·ªïng s·ªë ·∫£nh trong CSV: {len(df)}")
    print(f"‚úÖ S·ªë ·∫£nh x·ª≠ l√Ω th√†nh c√¥ng: {success_count}")
    print(f"‚ö†Ô∏è S·ªë ·∫£nh kh√¥ng t√¨m th·∫•y trong th∆∞ m·ª•c Local: {missing_files_count}")
    
    # Ki·ªÉm tra l·∫°i s·ªë l∆∞·ª£ng th·ª±c t·∫ø trong DB
    try:
        db_count = collection.count_documents({})
        print(f"üíæ S·ªë l∆∞·ª£ng document hi·ªán t·∫°i trong MongoDB: {db_count}")
    except:
        pass
        
    print("-" * 50)
    print("üëâ L∆ØU √ù CHO B∆Ø·ªöC TI·∫æP THEO:")
    print("1. V√†o MongoDB Atlas > Atlas Search.")
    print("2. T·∫°o ho·∫∑c C·∫≠p nh·∫≠t Index.")
    print("3. ƒê·∫£m b·∫£o field 'embedding' c√≥ 'numDimensions': 768 (DINOv2 Base).")
    print("="*50)
    
    client.close()

if __name__ == "__main__":
    main()